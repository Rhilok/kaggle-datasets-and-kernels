{"cells":[{"metadata":{"_uuid":"439f3c861e32608fae88d59208d0564853d96fa8"},"cell_type":"markdown","source":"## News headline"},{"metadata":{"trusted":true,"_uuid":"17db07111cfd70c7048073a457572b827edf379f"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nseed = 100","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0c8500571ae427c949fabd1050562792e0a98dc"},"cell_type":"markdown","source":"## Import dataset"},{"metadata":{"trusted":true,"_uuid":"6213fca90007770bb7264ec30996125151b26b69"},"cell_type":"code","source":"#path = 'dataset/'\npath = '../input/'\nnews = pd.read_json(path+ 'Sarcasm_Headlines_Dataset.json',lines=True)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d32be6eb4705907f26713284e10e0592b4f626d"},"cell_type":"markdown","source":"### Data analysis"},{"metadata":{"trusted":true,"_uuid":"d08292bdbbc4c94be82c66ac0da9097eb9b7bab2"},"cell_type":"code","source":"news['num_words'] = news['headline'].apply(lambda x: len(str(x).split()))\nprint('Maximum number of word',news['num_words'].max())\n\nprint('\\nSentence:\\n',news[news['num_words'] == 39]['headline'].values)\ntext = news[news['num_words'] == 39]['headline'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8d619b1e11bb3fabf42a99cb1a16387f8d271ef"},"cell_type":"markdown","source":"### Word tokenize\nA sentence or data split into words is called word tokenize"},{"metadata":{"trusted":true,"_uuid":"f582eb31def5102b18102c199e95d871a0f3a903"},"cell_type":"code","source":"# Word tokenize\nnlp = spacy.load('en')\ndoc = nlp(text[0])\n\n# List compresion method to get tokens\ntoken = [w.text for w in doc ]\nprint(token)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5571959392cb0767546a30910c75f42dfa304943"},"cell_type":"markdown","source":"### Punctuation\nSpacy library contains different punctuations, such as **Quotes, currency, punctuation** ect,\nIn above sentence we have seen inveted comma punctuation in the sentence and it will be considered as new word tocken, which is not usefull for our analysis. So we will remove that punctuation from sentence."},{"metadata":{"trusted":true,"_uuid":"e2b3197024985137a8819309f03199334bf4c879"},"cell_type":"code","source":"# Data preprocessing\n# Remove punctuation\nprint('Quotes:',spacy.lang.punctuation.LIST_QUOTES)\nprint('\\nPunctuations:',spacy.lang.punctuation.LIST_PUNCT)\n#print('\\n Currency:',spacy.lang.punctuation.LIST_CURRENCY)\n\n# list of punctuation contains most of punctuation, we will use only that for our analysis\npunc = [w.text for w in doc  if  w.is_punct ]\nprint('\\nPunctuation:',punc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e91cd9dac786c49ab0f4ed2e1ae9fb452f9a382f"},"cell_type":"markdown","source":"### Stopword\nIn this step we will remove stop words in dataset"},{"metadata":{"trusted":true,"_uuid":"38d6c15c2fe7497909769488d3d763b8b7fb8d6f"},"cell_type":"code","source":"stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\nprint('Number of stopwords is','-'*20,len(stopwords))\nprint('Ten stop words',list(stopwords)[:10])\nstop = [w.text for w in doc if w.is_stop]\nprint('*'*100,'\\n\\nStop word in sentence: ',stop)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12364b2a1e90c4eab5760d83d24abcb2b587b63e"},"cell_type":"markdown","source":"### Digit"},{"metadata":{"trusted":true,"_uuid":"ef5443ce25253cc7a63e7c3360d08f226454b0f6"},"cell_type":"code","source":"digit = [w.text for w in doc if w.is_digit]\nprint('Digit in sentence: ',digit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60dc366398a93386609a74d407f68e7578ac6bb1"},"cell_type":"markdown","source":"### Lemmatizing\nLemmetiztion is the process of retrieving the root word of the current word. Lemmatization is an essential process in NLP to bring different variants of a single word to one root word."},{"metadata":{"trusted":true,"_uuid":"cd13ba95bf0a6e47559e77d56efe35000174d0a2"},"cell_type":"code","source":"lemma = [w.lemma_ for w in doc]\nprint(lemma)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98b8cb931ab44b310149fc4e32ee2776fa4e4eb6"},"cell_type":"markdown","source":"### Named Entities\nA named entity is a \"real-world object\" that's assigned a name â€“ for example, a person, a country, a product or a book title."},{"metadata":{"trusted":true,"_uuid":"8484eeb840e91bd08a60d6eaec7eeed749e18ab6"},"cell_type":"code","source":"spacy.displacy.render(doc, style='ent', jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab9114356e1407d2ea50d9b9f0f5a73a7c2d2290"},"cell_type":"code","source":"df = pd.DataFrame(\n{\n    'token': [w.text for w in doc],\n    'lemma':[w.lemma_ for w in doc],\n    'POS': [w.pos_ for w in doc],\n    'TAG': [w.tag_ for w in doc],\n    'DEP': [w.dep_ for w in doc],\n    'is_stopword': [w.is_stop for w in doc],\n    'is_punctuation': [w.is_punct for w in doc],\n    'is_digit': [w.is_digit for w in doc],\n})\n\ndef highlight_True(s):\n    \"\"\"\n    Highlight True and False\n    \"\"\"\n    return ['background-color: yellow' if v else '' for v in s]\ndf.style.apply(highlight_True,subset=['is_stopword', 'is_punctuation', 'is_digit'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b73eabe159bf53953defb5eb9eba6651f5da58fc"},"cell_type":"markdown","source":"## Text preprocessing"},{"metadata":{"trusted":true,"_uuid":"45d419db2d51ab5607d79a28aac1b3d728d495a9"},"cell_type":"code","source":"def clean_text(df):\n    \"\"\"\n    Text preprocessing:\n    tokenize, make lower case,\n    Remove Stop word, punctuation, digit\n    lemmatize\n    \"\"\"\n    nlp = spacy.load('en')\n    for i in range(df.shape[0]):\n        doc = nlp(df['headline'][i])\n        # Word Tokenize\n        #token = [w.text for w in doc]\n        \n        # Make Lower case\n        # Remove Stop word, punctuation, digit and lemmatize\n        text = [w.lemma_.lower().strip() for w in doc \n               if not (w.is_stop |\n                    w.is_punct |\n                    w.is_digit)\n               ]\n        text = \" \".join(text)\n        \n        if i <5: print('Sentence:',i,text)\n        df['headline'][i] = text\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"412e9c81345e57cd740c8bee52c0cee590ad2b45"},"cell_type":"code","source":"news_df = clean_text(news)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d2b89e9a53b28124cc04f0f40a2091b39953f0f"},"cell_type":"markdown","source":"### Target valriable distribution"},{"metadata":{"trusted":true,"_uuid":"11e3b8c59a605a1f2c4c6d436f7c74fdc4b4fc01"},"cell_type":"code","source":"sns.countplot(news['is_sarcastic'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de444a44727f8da591887eb552c6f609d225499b"},"cell_type":"markdown","source":"### Tfidf vectorizer"},{"metadata":{"trusted":true,"_uuid":"46498ce88f42bcdcde5b141897552238bc90c0fc"},"cell_type":"code","source":"tf = TfidfVectorizer(analyzer='word',ngram_range=(1,3),max_features=5000)\nX = tf.fit_transform(news_df['headline'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2f574980265d98519293e46247de4adb1a8c61b"},"cell_type":"markdown","source":"### Model Selection"},{"metadata":{"trusted":true,"_uuid":"901178776416c1b82b12ae439dda35c99ff4350f"},"cell_type":"code","source":"y = news_df['is_sarcastic']\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.3, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c5c04bad26c54b764766e2c44b5ad4c2362b561"},"cell_type":"markdown","source":"### Model\nNaive bayes model"},{"metadata":{"trusted":true,"_uuid":"f2daf220e03ca9740ae1d4e501fa95f401976410"},"cell_type":"code","source":"nb = BernoulliNB()\nnb.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"407957ab53fb6a35b85d5af0521d141574b8e907"},"cell_type":"markdown","source":"## Model Evaluvation"},{"metadata":{"trusted":true,"_uuid":"31bd2601dcaa2e0776a2e1dd8c1152ce761ae8a6"},"cell_type":"code","source":"pred = nb.predict(X_valid)\nprint('Confusion matrix\\n',confusion_matrix(y_valid,pred))\nprint('Classification_report\\n',classification_report(y_valid,pred))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e0f4d9f-ea6b-4161-97a6-9a2dd3fcc0f3","_uuid":"98f12a18d5cc32ddfdf59e6177714bb3e050d2f1"},"cell_type":"markdown","source":"### Reciever Operating Charactaristics"},{"metadata":{"_cell_guid":"ab636b60-5923-4a33-bb26-14718cf5c3e0","_uuid":"19c789ce94744a856cc73c8d904ad63540392df2","trusted":true},"cell_type":"code","source":"proba = nb.predict_proba(X_valid)[:,1]\nfpr,tpr, threshold = roc_curve(y_valid,proba)\nauc_val = auc(fpr,tpr)\n\nplt.figure(figsize=(14,8))\nplt.title('Reciever Operating Charactaristics')\nplt.plot(fpr,tpr,'b',label = 'AUC = %0.2f' % auc_val)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e770c8cf7065a0005ef40b2dfd916f8f1491613"},"cell_type":"markdown","source":"### Thank you"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}